# Canonical model registry for GuardOS (text-only, no weights)
# Each entry includes metadata, quantization details, licensing, and system compatibility notes.

models:
  - id: gemma-0.27b-q4km
    display_name: Gemma 270M (Q4_K_M)
    family: gemma-270m
    size_params_b: 0.27
    quantizations:
      - file: gemma-0.27b-Q4_K_M.gguf
        note: Balanced quality/size for offline policy agents
    context: 2048
    runtime: [llama.cpp]
    licenses:
      spdx: "gemma-license"
      source: "https://ai.google.dev/gemma"
    sources:
      homepage: "https://ai.google.dev/gemma"
      mirrors: []
    sha256:
      gemma-0.27b-Q4_K_M.gguf: "FILL_ME_IN_SHA256"
    system_requirements:
      min_ram_gb: 2
      notes: "Good for classification, routing, and low-latency decision support."

  - id: tinyllama-1.1b-q4km
    display_name: TinyLlama 1.1B (Q4_K_M)
    family: tinyllama-1.1b
    size_params_b: 1.1
    quantizations:
      - file: TinyLlama-1.1B-Chat-v1.0-Q4_K_M.gguf
        note: Chat-tuned, compact general-purpose model
    context: 2048
    runtime: [llama.cpp]
    licenses:
      spdx: "Apache-2.0"
      source: "https://github.com/jzhang38/TinyLlama"
    sources:
      homepage: "https://huggingface.co/TinyLlama"
      mirrors: []
    sha256:
      TinyLlama-1.1B-Chat-v1.0-Q4_K_M.gguf: "FILL_ME_IN_SHA256"
    system_requirements:
      min_ram_gb: 3
      notes: "Handles small chat-style flows, summaries, simple question answering."

  - id: qwen1.5-0.5b-q4km
    display_name: Qwen 1.5‑0.5B (Q4_K_M)
    family: qwen1.5-0.5b
    size_params_b: 0.5
    quantizations:
      - file: Qwen1.5-0.5B-Chat-Q4_K_M.gguf
        note: Reasonable performance for short prompts
    context: 2048
    runtime: [llama.cpp]
    licenses:
      spdx: "Apache-2.0"
      source: "https://github.com/QwenLM/Qwen"
    sources:
      homepage: "https://huggingface.co/Qwen"
      mirrors: []
    sha256:
      Qwen1.5-0.5B-Chat-Q4_K_M.gguf: "FILL_ME_IN_SHA256"
    system_requirements:
      min_ram_gb: 2
      notes: "Best used for routing prompts, config helpers, and short completions."

  - id: phi-2-2.7b-q4km
    display_name: Phi‑2 2.7B (Q4_K_M)
    family: phi-2
    size_params_b: 2.7
    quantizations:
      - file: Phi-2-Q4_K_M.gguf
        note: Good accuracy for short tasks; heavier runtime
    context: 2048
    runtime: [llama.cpp]
    licenses:
      spdx: "custom-philicense"
      source: "https://github.com/microsoft/phi-2"
    sources:
      homepage: "https://huggingface.co/microsoft/phi-2"
      mirrors: []
    sha256:
      Phi-2-Q4_K_M.gguf: "FILL_ME_IN_SHA256"
    system_requirements:
      min_ram_gb: 4
      notes: "Better generation quality; useful for offline assistants and summaries."

# Global notes
notes:
  - "GuardOS does not ship model files. Users fetch them manually and verify SHA256."
  - "All models must support GGUF and run locally. No remote API inference."
  - "Runtime support starts with llama.cpp but may extend to others if policy-compliant."
